{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 \n",
    "### ASTR-324, Spring 2018, Ivezic & Juric, University of Washington\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "We learned this week that the posterior pdf for $b$, the tail probability when\n",
    "flipping a coin, with a flat prior in the range 0-1, is\n",
    "$$ p(b \\,|\\, k, N ) = C \\, b^k \\, (1-b)^{N-k}, $$\n",
    "where the normalization constant $C$ can be determined from the condition $\\int_0^1 p(b\\,|\\,k,N)\\, db = 1$.\n",
    "\n",
    "Get a coin, flip it 8 times and record the results.\n",
    "\n",
    "a) plot $p(b \\,|\\, k, N )$ after each flip. Every time show the previous (prior) $p(b)$\n",
    "and the posterior $p(b)$. \n",
    "\n",
    "b) After all 8 flips, what is the probability that your coin strongly prefers\n",
    "heads over tails: what is the probability $p(b<0.25\\,|\\, k, N )$)? \n",
    "\n",
    "\n",
    "## Problem 2\n",
    "\n",
    "Fit polynomials of up to the 5th order to the provided dataset.\n",
    "Use BIC to find the best model for this dataset.\n",
    "\n",
    "## Problem 3 \n",
    "\n",
    "Apply Bayesian Blocks Algorithm to period distribution (logP) of LINEAR variable stars. \n",
    "Plot histograms on both linear and log scale. Compare the result to classical (Knuth's) \n",
    "uniform bin width histogram. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# Homework 3: Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful definitions and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from astroML.datasets import fetch_LINEAR_geneva\n",
    "from astroML.plotting import setup_text_plots\n",
    "from astroML.plotting import hist\n",
    "setup_text_plots(fontsize=8, usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: the coin flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "np.random.seed(42)\n",
    "# results for my coin (0:head, 1: tail)\n",
    "flipData = [0, 0, 1, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a solution to Problem 1:\n",
    "![HW4](figures/HW4-binomial.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: use BIC to find the best polynomial model for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set() # set default plot styles\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "\n",
    "# this function computes polynomial models given some data x\n",
    "# and parameters theta\n",
    "def polynomial_fit(theta, x):\n",
    "    \"\"\"Polynomial model of degree (len(theta) - 1)\"\"\"\n",
    "    return sum(t * x ** n for (n, t) in enumerate(theta))\n",
    "\n",
    "# compute the data log-likelihood given a model\n",
    "def logL(theta, x, y, sigma_y, model=polynomial_fit):\n",
    "    \"\"\"Gaussian log-likelihood of the model at theta\"\"\"\n",
    "    y_fit = model(theta, x)\n",
    "    return sum(stats.norm.logpdf(*args)\n",
    "               for args in zip(y, y_fit, sigma_y))\n",
    "\n",
    "# a direct optimization approach is used to get best model \n",
    "# parameters (which minimize -logL)\n",
    "def best_theta(degree, x, y, sigma_y, model=polynomial_fit):\n",
    "    theta_0 = (degree + 1) * [0]\n",
    "    neg_logL = lambda theta: -logL(theta, x, y, sigma_y, model)\n",
    "    return optimize.fmin_bfgs(neg_logL, theta_0, disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate (noisy) data\n",
    "np.random.seed(0)\n",
    "Ndata = 22\n",
    "x = np.linspace(0, 3, Ndata)[1:-1]\n",
    "sigma_y = 0.1 + 0*x\n",
    "y = np.random.normal(np.sin(x) * x, sigma_y) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a solution to Problem 2:\n",
    "![HW4](figures/HW4-poly.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Compare Bayesian Blocks Algorithm and classsical histogram\n",
    "on a sample of periodic variable stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetch data\n",
    "data = fetch_LINEAR_geneva()\n",
    "x = data['logP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a solution to Problem 3:\n",
    "![HW4](figures/HW4-BB.jpg) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
